<?xml version="1.0" encoding="UTF-8"?><process version="10.5.000">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="10.5.000" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="retrieve" compatibility="10.5.000" expanded="true" height="68" name="Retrieve kafka-no-auth - 2" width="90" x="45" y="85">
        <parameter key="repository_entry" value="/Connections/kafka-no-auth - 2"/>
      </operator>
      <operator activated="true" class="multiply" compatibility="10.5.000" expanded="true" height="124" name="Multiply" width="90" x="246" y="85"/>
      <operator activated="true" class="retrieve" compatibility="10.5.000" expanded="true" height="68" name="Retrieve partner" width="90" x="45" y="238">
        <parameter key="repository_entry" value="//Local Repository/data/partner"/>
      </operator>
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="82" name="Write Kafka" width="90" x="447" y="289">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;Kafka Producer - Write Messages&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;TOPIC&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Kafka topic to write messages to&quot;,&#10;      &quot;value&quot;: &quot;ga_in&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;MESSAGE_COLUMN&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Column name containing the message to send&quot;,&#10;      &quot;value&quot;: &quot;message&quot;,&#10;      &quot;optional&quot;: true&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;KEY_COLUMN&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Column name for message key (optional)&quot;,&#10;      &quot;value&quot;: &quot;&quot;,&#10;      &quot;optional&quot;: true&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;SERIALIZATION_MODE&quot;,&#10;      &quot;type&quot;: &quot;category&quot;,&#10;      &quot;description&quot;: &quot;How to serialize messages&quot;,&#10;      &quot;categories&quot;: [&#10;        &quot;raw_string&quot;,&#10;        &quot;json&quot;,&#10;        &quot;text_object&quot;&#10;      ],&#10;      &quot;value&quot;: &quot;raw_string&quot;&#10;    }&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;kafka&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;data&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ],&#10;  &quot;outputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;through&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ]&#10;}.import json&#10;from kafka import KafkaProducer&#10;import pandas as pd&#10;from datetime import datetime&#10;&#10;def rm_main(kafka, data, parameters):&#10;    # Get broker from kafka connection object&#10;    BROKER = kafka\.get(&quot;host_ports&quot;, &quot;server\.crexdata\.eu:9192&quot;)&#10;    &#10;    # Get parameters&#10;    TOPIC = parameters[&quot;TOPIC&quot;]&#10;    MESSAGE_COLUMN = parameters\.get(&quot;MESSAGE_COLUMN&quot;, &quot;message&quot;)&#10;    KEY_COLUMN = parameters\.get(&quot;KEY_COLUMN&quot;, None)&#10;    SERIALIZATION_MODE = parameters\.get(&quot;SERIALIZATION_MODE&quot;, &quot;raw_string&quot;)&#10;    &#10;    print(f&quot;Writing to topic '{TOPIC}' on {BROKER}&quot;)&#10;    print(f&quot;Serialization mode: {SERIALIZATION_MODE}&quot;)&#10;    &#10;    # Configure producer based on serialization mode&#10;    if SERIALIZATION_MODE == &quot;json&quot;:&#10;        # JSON serialization - will convert Python objects to JSON&#10;        producer = KafkaProducer(&#10;            bootstrap_servers=BROKER,&#10;            value_serializer=lambda v: json\.dumps(v)\.encode('utf-8'),&#10;            key_serializer=lambda k: k\.encode('utf-8') if k else None&#10;        )&#10;    else:  # raw_string or text_object&#10;        # Raw string serialization - no JSON conversion&#10;        producer = KafkaProducer(&#10;            bootstrap_servers=BROKER,&#10;            value_serializer=lambda v: v\.encode('utf-8') if isinstance(v, str) else str(v)\.encode('utf-8'),&#10;            key_serializer=lambda k: k\.encode('utf-8') if isinstance(k, str) else (str(k)\.encode('utf-8') if k else None)&#10;        )&#10;    &#10;    messages_sent = 0&#10;    errors = []&#10;    &#10;    try:&#10;        # Handle different input types&#10;        if isinstance(data, pd\.DataFrame):&#10;            # Input is a DataFrame (Example Set)&#10;            for index, row in data\.iterrows():&#10;                try:&#10;                    # Get message content&#10;                    if MESSAGE_COLUMN in data\.columns:&#10;                        message_content = row[MESSAGE_COLUMN]&#10;                    else:&#10;                        # If no message column specified, use entire row as JSON&#10;                        message_content = row\.to_dict()&#10;                    &#10;                    # Handle the message based on serialization mode&#10;                    if SERIALIZATION_MODE == &quot;raw_string&quot;:&#10;                        # Send as raw string - no JSON encoding&#10;                        # If it's already a JSON string, send it as-is&#10;                        if isinstance(message_content, dict):&#10;                            message_to_send = json\.dumps(message_content)&#10;                        else:&#10;                            message_to_send = str(message_content)&#10;                    elif SERIALIZATION_MODE == &quot;json&quot;:&#10;                        # Send as JSON - will be encoded by serializer&#10;                        if isinstance(message_content, str):&#10;                            # Try to parse if it's a JSON string&#10;                            try:&#10;                                message_to_send = json\.loads(message_content)&#10;                            except:&#10;                                message_to_send = message_content&#10;                        else:&#10;                            message_to_send = message_content&#10;                    else:  # text_object&#10;                        # Treat as plain text&#10;                        message_to_send = str(message_content)&#10;                    &#10;                    # Get key if specified&#10;                    key = None&#10;                    if KEY_COLUMN and KEY_COLUMN in data\.columns:&#10;                        key = str(row[KEY_COLUMN])&#10;                    &#10;                    # Send to Kafka&#10;                    future = producer\.send(TOPIC, value=message_to_send, key=key)&#10;                    result = future\.get(timeout=10)&#10;                    messages_sent += 1&#10;                    &#10;                    if messages_sent % 100 == 0:&#10;                        print(f&quot;Sent {messages_sent} messages\.\.\.&quot;)&#10;                        &#10;                except Exception as e:&#10;                    error_msg = f&quot;Error sending row {index}: {str(e)}&quot;&#10;                    errors\.append(error_msg)&#10;                    print(error_msg)&#10;                    &#10;        elif isinstance(data, str):&#10;            # Input is a text object or string&#10;            # Send as single message&#10;            if SERIALIZATION_MODE == &quot;raw_string&quot;:&#10;                message_to_send = data&#10;            elif SERIALIZATION_MODE == &quot;json&quot;:&#10;                try:&#10;                    message_to_send = json\.loads(data)&#10;                except:&#10;                    message_to_send = data&#10;            else:&#10;                message_to_send = data&#10;            &#10;            future = producer\.send(TOPIC, value=message_to_send)&#10;            result = future\.get(timeout=10)&#10;            messages_sent += 1&#10;            &#10;        elif isinstance(data, list):&#10;            # Input is a list of messages&#10;            for item in data:&#10;                try:&#10;                    if SERIALIZATION_MODE == &quot;raw_string&quot;:&#10;                        if isinstance(item, dict):&#10;                            message_to_send = json\.dumps(item)&#10;                        else:&#10;                            message_to_send = str(item)&#10;                    elif SERIALIZATION_MODE == &quot;json&quot;:&#10;                        message_to_send = item&#10;                    else:&#10;                        message_to_send = str(item)&#10;                    &#10;                    future = producer\.send(TOPIC, value=message_to_send)&#10;                    result = future\.get(timeout=10)&#10;                    messages_sent += 1&#10;                    &#10;                    if messages_sent % 100 == 0:&#10;                        print(f&quot;Sent {messages_sent} messages\.\.\.&quot;)&#10;                        &#10;                except Exception as e:&#10;                    error_msg = f&quot;Error sending item: {str(e)}&quot;&#10;                    errors\.append(error_msg)&#10;                    print(error_msg)&#10;        &#10;        # Flush remaining messages&#10;        producer\.flush()&#10;        &#10;    except Exception as e:&#10;        print(f&quot;Producer error: {e}&quot;)&#10;        import traceback&#10;        traceback\.print_exc()&#10;    finally:&#10;        producer\.close()&#10;        &#10;    print(f&quot;\\n{'='*50}&quot;)&#10;    print(f&quot;Successfully sent {messages_sent} messages to topic '{TOPIC}'&quot;)&#10;    if errors:&#10;        print(f&quot;Encountered {len(errors)} errors&quot;)&#10;    &#10;    # Return status as DataFrame&#10;    status_df = pd\.DataFrame({&#10;        'topic': [TOPIC],&#10;        'messages_sent': [messages_sent],&#10;        'errors': [len(errors)],&#10;        'status': ['Success' if len(errors) == 0 else 'Partial Success']&#10;    })&#10;    &#10;    return status_df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="TOPIC" value="RM_TEST"/>
        <parameter key="MESSAGE_COLUMN" value="message"/>
        <parameter key="KEY_COLUMN" value=""/>
        <parameter key="SERIALIZATION_MODE" value="json"/>
      </operator>
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="68" name="Read Kafka" width="90" x="447" y="136">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;Kafka Consumer - Read All&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;TOPIC&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Kafka topic to consume messages from&quot;,&#10;      &quot;value&quot;: &quot;ga_out&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;READ_MODE&quot;,&#10;      &quot;type&quot;: &quot;category&quot;,&#10;      &quot;description&quot;: &quot;Reading mode: all existing messages or listen for new&quot;,&#10;      &quot;categories&quot;: [&#10;        &quot;all_existing&quot;,&#10;        &quot;new_only&quot;,&#10;        &quot;last_n_messages&quot;&#10;      ],&#10;      &quot;value&quot;: &quot;all_existing&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;MAX_MESSAGES&quot;,&#10;      &quot;type&quot;: &quot;integer&quot;,&#10;      &quot;description&quot;: &quot;Maximum messages to read (0 = unlimited)&quot;,&#10;      &quot;value&quot;: 0&#10;    }&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;through&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ],&#10;  &quot;outputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;through&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ]&#10;}.import json&#10;from kafka import KafkaConsumer, TopicPartition&#10;import pandas as pd&#10;from datetime import datetime&#10;&#10;def rm_main(data, parameters):&#10;    # Get parameters from RapidMiner&#10;    BROKER = data[&quot;cluster_config\.host_ports&quot;]&#10;    print(BROKER)&#10;    TOPIC = parameters[&quot;TOPIC&quot;]&#10;    READ_MODE = parameters\.get(&quot;READ_MODE&quot;, &quot;all_existing&quot;)&#10;    MAX_MESSAGES = parameters\.get(&quot;MAX_MESSAGES&quot;, 0)&#10;    TIMEOUT_SECONDS = parameters\.get(&quot;TIMEOUT_SECONDS&quot;, 10)&#10;    &#10;    messages = []&#10;    &#10;    try:&#10;        if READ_MODE == &quot;all_existing&quot;:&#10;            # Create consumer to read ALL messages from beginning&#10;            consumer = KafkaConsumer(&#10;                bootstrap_servers=BROKER,&#10;                auto_offset_reset='earliest',&#10;                enable_auto_commit=False,&#10;                value_deserializer=lambda m: json\.loads(m\.decode('utf-8')),&#10;                consumer_timeout_ms=TIMEOUT_SECONDS * 1000&#10;            )&#10;            &#10;            # Get all partitions for the topic&#10;            partitions = consumer\.partitions_for_topic(TOPIC)&#10;            if partitions:&#10;                # Assign all partitions&#10;                topic_partitions = [TopicPartition(TOPIC, p) for p in partitions]&#10;                consumer\.assign(topic_partitions)&#10;                &#10;                # Seek to beginning of all partitions&#10;                consumer\.seek_to_beginning()&#10;                &#10;                print(f&quot;Reading all messages from topic '{TOPIC}' on {BROKER}\.\.\.&quot;)&#10;                print(f&quot;Found {len(partitions)} partition(s)&quot;)&#10;                &#10;                # Read all messages&#10;                message_count = 0&#10;                for message in consumer:&#10;                    msg_data = {&#10;                        'message': json\.dumps(message\.value) if message\.value else &quot;&quot;,&#10;                        'timestamp': datetime\.fromtimestamp(message\.timestamp / 1000)\.isoformat() if message\.timestamp else &quot;&quot;,&#10;                        'partition': message\.partition,&#10;                        'offset': message\.offset,&#10;                        'topic': message\.topic,&#10;                        'key': message\.key\.decode('utf-8') if message\.key else None&#10;                    }&#10;                    messages\.append(msg_data)&#10;                    message_count += 1&#10;                    &#10;                    if MAX_MESSAGES &gt; 0 and message_count &gt;= MAX_MESSAGES:&#10;                        print(f&quot;Reached maximum message limit: {MAX_MESSAGES}&quot;)&#10;                        break&#10;                    &#10;                    if message_count % 100 == 0:&#10;                        print(f&quot;Read {message_count} messages\.\.\.&quot;)&#10;                        &#10;        elif READ_MODE == &quot;new_only&quot;:&#10;            # Read only new messages (from latest)&#10;            consumer = KafkaConsumer(&#10;                TOPIC,&#10;                bootstrap_servers=BROKER,&#10;                auto_offset_reset='latest',&#10;                enable_auto_commit=True,&#10;                value_deserializer=lambda m: json\.loads(m\.decode('utf-8')),&#10;                consumer_timeout_ms=TIMEOUT_SECONDS * 1000&#10;            )&#10;            &#10;            print(f&quot;Listening for new messages from topic '{TOPIC}' on {BROKER}\.\.\.&quot;)&#10;            &#10;            message_count = 0&#10;            for message in consumer:&#10;                msg_data = {&#10;                    'message': json\.dumps(message\.value) if message\.value else &quot;&quot;,&#10;                    'timestamp': datetime\.fromtimestamp(message\.timestamp / 1000)\.isoformat() if message\.timestamp else &quot;&quot;,&#10;                    'partition': message\.partition,&#10;                    'offset': message\.offset,&#10;                    'topic': message\.topic,&#10;                    'key': message\.key\.decode('utf-8') if message\.key else None&#10;                }&#10;                messages\.append(msg_data)&#10;                message_count += 1&#10;                &#10;                if MAX_MESSAGES &gt; 0 and message_count &gt;= MAX_MESSAGES:&#10;                    break&#10;                    &#10;        elif READ_MODE == &quot;last_n_messages&quot;:&#10;            # Read last N messages&#10;            consumer = KafkaConsumer(&#10;                bootstrap_servers=BROKER,&#10;                enable_auto_commit=False,&#10;                value_deserializer=lambda m: json\.loads(m\.decode('utf-8')),&#10;                consumer_timeout_ms=TIMEOUT_SECONDS * 1000  # Important: Add timeout here&#10;            )&#10;            &#10;            # Get partitions&#10;            partitions = consumer\.partitions_for_topic(TOPIC)&#10;            if partitions:&#10;                all_messages = []  # Collect from all partitions&#10;                &#10;                for partition in partitions:&#10;                    tp = TopicPartition(TOPIC, partition)&#10;                    consumer\.assign([tp])&#10;                    &#10;                    # Get the last offset&#10;                    end_offset = consumer\.end_offsets([tp])[tp]&#10;                    &#10;                    if end_offset == 0:  # Empty partition&#10;                        print(f&quot;Partition {partition} is empty&quot;)&#10;                        continue&#10;                    &#10;                    # Calculate start offset for this partition&#10;                    messages_to_read = MAX_MESSAGES if MAX_MESSAGES &gt; 0 else end_offset&#10;                    start_offset = max(0, end_offset - messages_to_read)&#10;                    &#10;                    # Seek to start position&#10;                    consumer\.seek(tp, start_offset)&#10;                    &#10;                    print(f&quot;Partition {partition}: Reading from offset {start_offset} to {end_offset-1}&quot;)&#10;                    &#10;                    # Read messages from this partition&#10;                    partition_messages = []&#10;                    try:&#10;                        for message in consumer:&#10;                            # Check if we've reached the end of this partition&#10;                            if message\.offset &gt;= end_offset:&#10;                                break&#10;                            &#10;                            msg_data = {&#10;                                'message': json\.dumps(message\.value) if message\.value else &quot;&quot;,&#10;                                'timestamp': datetime\.fromtimestamp(message\.timestamp / 1000)\.isoformat() if message\.timestamp else &quot;&quot;,&#10;                                'partition': message\.partition,&#10;                                'offset': message\.offset,&#10;                                'topic': message\.topic,&#10;                                'key': message\.key\.decode('utf-8') if message\.key else None&#10;                            }&#10;                            partition_messages\.append(msg_data)&#10;                            &#10;                            # Stop if we've read enough messages from this partition&#10;                            if len(partition_messages) &gt;= messages_to_read:&#10;                                break&#10;                    except StopIteration:&#10;                        # Consumer timeout - no more messages in this partition&#10;                        print(f&quot;Finished reading partition {partition} (timeout or end reached)&quot;)&#10;                    &#10;                    all_messages\.extend(partition_messages)&#10;                    print(f&quot;Read {len(partition_messages)} messages from partition {partition}&quot;)&#10;                &#10;                # Sort all messages by timestamp and take last N&#10;                if all_messages and MAX_MESSAGES &gt; 0:&#10;                    all_messages\.sort(key=lambda x: x['timestamp'])&#10;                    messages = all_messages[-MAX_MESSAGES:]&#10;                    print(f&quot;Selected last {len(messages)} messages across all partitions&quot;)&#10;                else:&#10;                    messages = all_messages&#10;        &#10;    except Exception as e:&#10;        print(f&quot;Error occurred: {e}&quot;)&#10;        import traceback&#10;        traceback\.print_exc()&#10;    finally:&#10;        if 'consumer' in locals():&#10;            consumer\.close()&#10;        print(f&quot;\\nTotal messages collected: {len(messages)}&quot;)&#10;    &#10;    # Return as DataFrame for RapidMiner&#10;    if messages:&#10;        df = pd\.DataFrame(messages)&#10;    else:&#10;        # Return empty DataFrame with expected columns if no messages&#10;        df = pd\.DataFrame(columns=['message', 'timestamp', 'partition', 'offset', 'topic', 'key'])&#10;    &#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="TOPIC" value="RM_TEST"/>
        <parameter key="READ_MODE" value="last_n_messages"/>
        <parameter key="MAX_MESSAGES" value="4"/>
      </operator>
      <operator activated="false" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="68" name="Topic List" width="90" x="447" y="34">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;Kafka Topic Lister&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#10;&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;kafka&quot;,&#10;      &quot;type&quot;: &quot;table&quot;,&#10;      &quot;optional&quot;: true&#10;    }&#10;  ],&#10;  &quot;outputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;topics&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ]&#10;}.from kafka import KafkaConsumer&#10;from kafka\.admin import KafkaAdminClient&#10;import pandas as pd&#10;&#10;def rm_main(data, parameters):&#10;    # Get broker from data or parameters&#10;    BROKER = data\.get(&quot;cluster_config\.host_ports&quot;, parameters\.get(&quot;BROKER&quot;, &quot;server\.crexdata\.eu:9192&quot;))&#10;    INCLUDE_INTERNAL = parameters\.get(&quot;INCLUDE_INTERNAL&quot;, False)&#10;    TIMEOUT_SECONDS = parameters\.get(&quot;TIMEOUT_SECONDS&quot;, 10)&#10;    &#10;    print(f&quot;Connecting to Kafka broker: {BROKER}&quot;)&#10;    &#10;    topics_list = []&#10;    &#10;    try:&#10;        # Method 1: Using KafkaConsumer (simpler, works in most cases)&#10;        consumer = KafkaConsumer(&#10;            bootstrap_servers=BROKER,&#10;            consumer_timeout_ms=TIMEOUT_SECONDS * 1000&#10;        )&#10;        &#10;        # Get all topics&#10;        topics = consumer\.topics()&#10;        &#10;        if topics:&#10;            print(f&quot;Found {len(topics)} topics in the cluster&quot;)&#10;            &#10;            # Filter internal topics if needed&#10;            for topic in sorted(topics):&#10;                # Internal Kafka topics usually start with '__'&#10;                if not INCLUDE_INTERNAL and topic\.startswith('__'):&#10;                    continue&#10;                &#10;                # Get topic metadata&#10;                partitions = consumer\.partitions_for_topic(topic)&#10;                num_partitions = len(partitions) if partitions else 0&#10;                &#10;                topic_info = {&#10;                    'topic_name': topic,&#10;                    'partitions': num_partitions,&#10;                    'is_internal': topic\.startswith('__')&#10;                }&#10;                topics_list\.append(topic_info)&#10;                &#10;            print(f&quot;Returning {len(topics_list)} topics (internal topics {'included' if INCLUDE_INTERNAL else 'excluded'})&quot;)&#10;        else:&#10;            print(&quot;No topics found in the cluster&quot;)&#10;            &#10;        consumer\.close()&#10;        &#10;    except Exception as e:&#10;        print(f&quot;Error connecting to Kafka: {e}&quot;)&#10;        &#10;        # Try alternative method with KafkaAdminClient&#10;        try:&#10;            print(&quot;Trying alternative method with KafkaAdminClient\.\.\.&quot;)&#10;            admin_client = KafkaAdminClient(&#10;                bootstrap_servers=BROKER,&#10;                request_timeout_ms=TIMEOUT_SECONDS * 1000&#10;            )&#10;            &#10;            # List topics&#10;            topic_metadata = admin_client\.list_topics()&#10;            &#10;            for topic in sorted(topic_metadata):&#10;                if not INCLUDE_INTERNAL and topic\.startswith('__'):&#10;                    continue&#10;                    &#10;                topic_info = {&#10;                    'topic_name': topic,&#10;                    'partitions': 0,  # Would need additional call to get partition count&#10;                    'is_internal': topic\.startswith('__')&#10;                }&#10;                topics_list\.append(topic_info)&#10;            &#10;            admin_client\.close()&#10;            &#10;        except Exception as e2:&#10;            print(f&quot;Alternative method also failed: {e2}&quot;)&#10;    &#10;    # Create DataFrame&#10;    if topics_list:&#10;        df = pd\.DataFrame(topics_list)&#10;        # Sort by topic name&#10;        df = df\.sort_values('topic_name')\.reset_index(drop=True)&#10;    else:&#10;        # Return empty DataFrame with expected columns&#10;        df = pd\.DataFrame(columns=['topic_name', 'partitions', 'is_internal'])&#10;    &#10;    print(f&quot;\\n{'='*50}&quot;)&#10;    print(f&quot;Total topics found: {len(df)}&quot;)&#10;    if len(df) &gt; 0:&#10;        print(&quot;\\nTopic list:&quot;)&#10;        for idx, row in df\.iterrows():&#10;            print(f&quot;  - {row['topic_name']} ({row['partitions']} partitions)&quot;)&#10;    &#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="false" class="filter_examples" compatibility="10.5.000" expanded="true" height="103" name="Filter Examples" width="90" x="581" y="34">
        <parameter key="parameter_expression" value=""/>
        <parameter key="condition_class" value="custom_filters"/>
        <parameter key="invert_filter" value="false"/>
        <list key="filters_list">
          <parameter key="filters_entry_key" value="topic_name.contains.test"/>
        </list>
        <parameter key="filters_logic_and" value="true"/>
        <parameter key="filters_check_metadata" value="true"/>
      </operator>
      <connect from_op="Retrieve kafka-no-auth - 2" from_port="output" to_op="Multiply" to_port="input"/>
      <connect from_op="Multiply" from_port="output 2" to_op="Read Kafka" to_port="through"/>
      <connect from_op="Multiply" from_port="output 3" to_op="Write Kafka" to_port="kafka"/>
      <connect from_op="Retrieve partner" from_port="output" to_op="Write Kafka" to_port="data"/>
      <connect from_op="Write Kafka" from_port="through" to_port="result 1"/>
      <connect from_op="Read Kafka" from_port="through" to_port="result 2"/>
      <connect from_op="Topic List" from_port="topics" to_op="Filter Examples" to_port="example set input"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="105"/>
      <portSpacing port="sink_result 3" spacing="63"/>
    </process>
  </operator>
</process>
