<?xml version="1.0" encoding="UTF-8"?><process version="10.5.000">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.4.000" expanded="true" name="Process" origin="GENERATED_TUTORIAL">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="retrieve" compatibility="10.5.000" expanded="true" height="68" name="Retrieve kafka-no-auth - 2" width="90" x="45" y="136">
        <parameter key="repository_entry" value="/Connections/kafka-no-auth - 2"/>
      </operator>
      <operator activated="true" class="kafka_connector:read_kafka_topic" compatibility="0.3.002" expanded="true" height="82" name="Read Kafka Topic" width="90" x="179" y="136">
        <parameter key="kafka_topic" value="UPB-CREXDATA-Flooding-Optimizer-Input"/>
        <parameter key="offset_strategy" value="earliest"/>
        <parameter key="retrieval_time_out" value="5"/>
        <parameter key="get_all" value="true"/>
        <parameter key="number_of_records" value="100"/>
        <parameter key="collection_strategy" value="duration"/>
        <parameter key="counter" value="100"/>
        <parameter key="time_out" value="120"/>
        <parameter key="polling_time_out" value="5"/>
        <parameter key="api_timeout" value="10"/>
        <description align="center" color="transparent" colored="false" width="126">UPB-CREXDATA-Flooding-Optimizer-Input</description>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="10.1.002" expanded="true" height="103" name="Generate ga_config" width="90" x="380" y="136">
        <parameter key="script" value="import pandas&#10;import json&#10;&#10;# rm_main is a mandatory function, &#10;# the number of arguments has to be the number of input ports (can be none),&#10;#     or the number of input ports plus one if &quot;use macros&quot; parameter is set&#10;# if you want to use macros, use this instead and check &quot;use macros&quot; parameter:&#10;#def rm_main(data,macros):&#10;def rm_main(data):&#10;    print('Processing optimization request...')&#10;    &#10;    # Get the incoming data from the last row&#10;    incoming_data_str = data.iloc[-1]['value']&#10;    print(f&quot;Incoming data type: {type(incoming_data_str)}&quot;)&#10;    &#10;    # Parse the JSON string&#10;    incoming_data = json.loads(incoming_data_str)&#10;    &#10;    # Extract required values&#10;    optimizer_request_id = incoming_data['optimizerRequestID']&#10;    fitness_score = incoming_data['fitnessScore']&#10;    barriers = incoming_data['barriers']&#10;    &#10;    # Build ga_chromosome_def dynamically from barriers&#10;    ga_chromosome_def = []&#10;    for barrier in barriers:&#10;        chromosome = {&#10;            &quot;name&quot;: barrier['name'],&#10;            &quot;type&quot;: &quot;float&quot;,&#10;            &quot;lower&quot;: barrier['minHeight'],&#10;            &quot;upper&quot;: barrier['maxHeight'],&#10;            &quot;sigma&quot;: 0.1&#10;        }&#10;        ga_chromosome_def.append(chromosome)&#10;    &#10;    # Build the ga_config message&#10;    ga_config = {&#10;        &quot;process_id&quot;: optimizer_request_id,&#10;        &quot;experiment_folder&quot;: &quot;optimizer_runs&quot;,&#10;        &quot;timepoint&quot;: &quot;now&quot;,&#10;        &quot;ga_config.json&quot;: {&#10;            &quot;distance_type&quot;: &quot;l1&quot;,&#10;            &quot;termination_crit&quot;: &quot;genmax&quot;, # was fitmin&#10;            &quot;termination_args&quot;: 3, # was fitness_score&#10;            &quot;pop_num&quot;: 4,&#10;            &quot;crossover_prob&quot;: 0.75,&#10;            &quot;mutation_prob&quot;: 0.5,&#10;            &quot;tournament_size&quot;: 3,&#10;            &quot;ga_chromosome_def&quot;: ga_chromosome_def&#10;        }&#10;    }&#10;    &#10;    # Convert to JSON string for output&#10;    ga_config_json = json.dumps(ga_config, indent=2)&#10;    &#10;    # Print for verification&#10;    print(&quot;Generated ga_config:&quot;)&#10;    print(ga_config_json)&#10;    print(f&quot;\nNumber of barriers processed: {len(barriers)}&quot;)&#10;    for i, barrier in enumerate(barriers, 1):&#10;        print(f&quot;Barrier {i}: {barrier['name']} (height range: {barrier['minHeight']}-{barrier['maxHeight']})&quot;)&#10;    &#10;    # Create output dataframe with the ga_config&#10;    output_data = pandas.DataFrame({&#10;        'config_topic': [ga_config_json]&#10;    })&#10;    &#10;    return output_data"/>
        <parameter key="notebook_cell_tag_filter" value=""/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="use_macros" value="false"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="10.1.002" expanded="true" height="103" name="Execute Python (2)" width="90" x="581" y="136">
        <parameter key="script" value="import threading&#10;import random&#10;import time&#10;import math&#10;import csv&#10;import json&#10;import sys&#10;import pickle&#10;import logging&#10;import os&#10;import requests&#10;&#10;import numpy as np&#10;import pandas as pd&#10;&#10;from deap import base&#10;from deap import creator&#10;from deap import tools&#10;from deap import algorithms&#10;&#10;import ga_utils&#10;&#10;# Option 1: Use kafka-python-ng (recommended - install: pip install kafka-python-ng)&#10;from kafka import KafkaConsumer&#10;from kafka import KafkaProducer&#10;&#10;# Option 2: Use confluent-kafka (uncomment below and comment above)&#10;# from confluent_kafka import Consumer, Producer&#10;# Note: confluent-kafka requires different implementation patterns&#10;&#10;&#10;###&#10;### Initiate local kafka by commencing 'docker run -p 9092:9092 apache/kafka:latest'&#10;###&#10;&#10;experiment_folder = os.path.join('./', 'exp_folder')&#10;logging.basicConfig(&#10;    format='%(message)s',&#10;    filename=os.path.join(experiment_folder, &quot;generations.log&quot;),&#10;    level=logging.INFO&#10;)&#10;transformer = None&#10;&#10;&#10;class Transformer:&#10;&#10;    def __init__(self, ga_params, clf=None, scaler=None):&#10;        self.ga_params = ga_params&#10;&#10;    def mutate(self, population, indpb):&#10;        &quot;&quot;&quot;&#10;        Mutates the values in list individual with probability indpb&#10;        &quot;&quot;&quot;&#10;        for i, param in enumerate(self.ga_params):&#10;            individual = param.mutate(population[i], mu=0, indpb=indpb)&#10;            population[i] = individual&#10;&#10;        return population,&#10;&#10;    def cxUniform(self, ind1, ind2, indpb):&#10;        for _ in range(100):&#10;            c1, c2 = tools.cxUniform(ind1, ind2, indpb)&#10;&#10;        return (c1, c2)&#10;&#10;    def random_params(self):&#10;        draws = []&#10;        for p in self.ga_params:&#10;            draws.append(round(p.randomDraw(), 2))&#10;&#10;        return draws&#10;&#10;    def parse_init_params(self, params_file):&#10;        init_params = []&#10;        with open(params_file) as f_in:&#10;            reader = csv.reader(f_in)&#10;            header = next(reader)&#10;            for row in reader:&#10;                init_params.append(dict(zip(header, row)))&#10;        return init_params&#10;&#10;&#10;def printf(val):&#10;    print(val)&#10;    sys.stdout.flush()&#10;&#10;&#10;def obj_func(x):&#10;    &quot;&quot;&quot;Placeholder objective function&quot;&quot;&quot;&#10;    return 0&#10;&#10;&#10;def num(s):&#10;    &quot;&quot;&quot;Convert string to int or float&quot;&quot;&quot;&#10;    try:&#10;        return int(s)&#10;    except ValueError:&#10;        return float(s)&#10;&#10;&#10;def create_list_of_json_strings(list_of_lists, super_delim=&quot;;&quot;):&#10;    &quot;&quot;&quot;Create string of ; separated jsonified maps&quot;&quot;&quot;&#10;    res = []&#10;    global transformer&#10;    for l in list_of_lists:&#10;        jmap = {}&#10;        for i, p in enumerate(transformer.ga_params):&#10;            jmap[p.name] = l[i]&#10;&#10;        jstring = json.dumps(jmap)&#10;        res.append(jstring)&#10;&#10;    return (super_delim.join(res))&#10;&#10;&#10;def queue_map(obj_func, pops):&#10;    &quot;&quot;&quot;&#10;    Send population to server and receive fitness scores from HTTP response&#10;    &quot;&quot;&quot;&#10;    global proc_id&#10;    &#10;    if not pops:&#10;        return [(999.0,) for _ in pops]&#10;&#10;    url = &quot;https://server.crexdata.eu/webapi/DEFAULT/api/v1/services/barrieroptimization/create-barrier-height-simulations/?example=example&quot;&#10;&#10;    payload = {&#10;        &quot;data&quot;: [&#10;            {&#10;                &quot;optimizerRequestID&quot;: proc_id,&#10;                &quot;parameters&quot;: json.dumps(pops)&#10;            }&#10;        ]&#10;    }&#10;    headers = {'Content-type': 'application/json'}&#10;&#10;    print(f&quot;Submitting {len(pops)} individuals...&quot;)&#10;    &#10;    try:&#10;        r = requests.post(url, json=payload, headers=headers, timeout=900)&#10;        &#10;        print(f&quot;Status Code: {r.status_code}&quot;)&#10;        print(f&quot;Response Body: {r.text}&quot;)&#10;        &#10;        if r.status_code == 200:&#10;            response_data = r.json()&#10;            &#10;            # Extract from HTTP response&#10;            if 'data' in response_data and len(response_data['data']) &gt; 0:&#10;                data_item = response_data['data'][0]&#10;                &#10;                # Verify optimizer ID&#10;                if data_item.get('optimizerRequestID') == proc_id:&#10;                    result = data_item.get(&quot;results&quot;)&#10;                    &#10;                    if result:&#10;                        # Parse if string: &quot;[0.214, 0.214, ...]&quot;&#10;                        if isinstance(result, str):&#10;                            result = json.loads(result)&#10;                        &#10;                        print(f&quot;✓ Received {len(result)} fitness scores from HTTP: {result}&quot;)&#10;                        &#10;                        # CRITICAL: Convert to list of tuples for DEAP&#10;                        fitness_tuples = [(float(score),) for score in result]&#10;                        &#10;                        print(f&quot;✓ Converted to DEAP format: {fitness_tuples}&quot;)&#10;                        return fitness_tuples&#10;                    else:&#10;                        print(&quot;⚠ No results field in response&quot;)&#10;                else:&#10;                    print(f&quot;⚠ Optimizer ID mismatch: expected {proc_id}, got {data_item.get('optimizerRequestID')}&quot;)&#10;            else:&#10;                print(&quot;⚠ Invalid response structure&quot;)&#10;        else:&#10;            print(f&quot;⚠ HTTP error: {r.status_code}&quot;)&#10;        &#10;        # If we get here, something went wrong&#10;        print(&quot;✗ Failed to get fitness scores, returning penalty values&quot;)&#10;        return [(999.0,) for _ in pops]&#10;        &#10;    except Exception as e:&#10;        print(f&quot;✗ Error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return [(999.0,) for _ in pops]&#10;&#10;&#10;def make_random_parameters():&#10;    &quot;&quot;&quot;Performs initial random draw on each parameter&quot;&quot;&quot;&#10;    return transformer.random_params()&#10;&#10;&#10;def custom_mutate(individual, indpb):&#10;    &quot;&quot;&quot;Mutates the values in list individual with probability indpb&quot;&quot;&quot;&#10;    return transformer.mutate(individual, indpb)&#10;&#10;&#10;def cxUniform(ind1, ind2, indpb):&#10;    return transformer.cxUniform(ind1, ind2, indpb)&#10;&#10;&#10;def timestamp(scores):&#10;    return str(time.time())&#10;&#10;&#10;def eaSimpleExtended(population, toolbox, cxpb, mutpb, term, ngen, stats=None,&#10;                     halloffame=None, verbose=__debug__, checkpoint=None):&#10;    &quot;&quot;&quot;Extended evolutionary algorithm with checkpointing&quot;&quot;&quot;&#10;    visited_inds = {}&#10;    &#10;    if checkpoint:&#10;        with open(checkpoint, &quot;rb&quot;) as cp_file:&#10;            cp = pickle.load(cp_file)&#10;        population = cp[&quot;population&quot;]&#10;        halloffame = cp[&quot;halloffame&quot;]&#10;        logbook = cp[&quot;logbook&quot;]&#10;        random.setstate(cp[&quot;rndstate&quot;])&#10;    else:&#10;        logbook = tools.Logbook()&#10;        logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])&#10;&#10;    # Evaluate the individuals with an invalid fitness&#10;    invalid_ind = [ind for ind in population &#10;                   if (not ind.fitness.valid) and (str(ind) not in visited_inds)]&#10;    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)&#10;    &#10;    for ind, fit in zip(invalid_ind, fitnesses):&#10;        ind.fitness.values = fit&#10;        visited_inds[str(ind)] = fit&#10;&#10;    if halloffame is not None:&#10;        halloffame.update(population)&#10;    &#10;    record = stats.compile(population) if stats else {}&#10;    logbook.record(gen=0, nevals=len(invalid_ind), **record)&#10;    &#10;    if verbose:&#10;        for p in population:&#10;            logging.debug(&quot;0, {}, {}, {}&quot;.format(0, p, p.fitness))&#10;    &#10;    logging.info(&quot;Initial Generation fitness variance = {}&quot;.format(&#10;        math.pow(float(logbook.select(&quot;std&quot;)[-1]), 2)))&#10;    logging.debug(&quot;Term crit type: {}&quot;.format(type(ngen)))&#10;    &#10;    if term == 'genmax':  # Run for ngens&#10;        logging.debug(&quot;Following normal termination criterion process.&quot;)&#10;        &#10;        for gen in range(1, ngen + 1):&#10;            offspring = toolbox.select(population, len(population))&#10;            offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)&#10;            &#10;            for ind in offspring:&#10;                if str(ind) in visited_inds:&#10;                    ind.fitness.values = visited_inds[str(ind)]&#10;            &#10;            invalid_ind = [ind for ind in offspring &#10;                          if (not ind.fitness.valid) and (str(ind) not in visited_inds)]&#10;            fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)&#10;            &#10;            for ind, fit in zip(invalid_ind, fitnesses):&#10;                ind.fitness.values = fit&#10;                visited_inds[str(ind)] = fit&#10;&#10;            if halloffame is not None:&#10;                halloffame.update(offspring)&#10;&#10;            population[:] = offspring&#10;&#10;            record = stats.compile(population) if stats else {}&#10;            logbook.record(gen=gen, nevals=len(invalid_ind), **record)&#10;            &#10;            cp = dict(population=population, generation=gen, halloffame=halloffame,&#10;                     logbook=logbook, rndstate=random.getstate())&#10;            &#10;            with open(checkpoint_file, &quot;wb&quot;) as cp_file:&#10;                pickle.dump(cp, cp_file)&#10;            &#10;            logging.info(&quot;Generation {} Stored at {}&quot;.format(&#10;                gen, time.strftime(&quot;%H:%M:%S&quot;, time.localtime())))&#10;            &#10;            if verbose:&#10;                printf(&quot;Logbookstream: {}\nhalloffame: {}\n&quot;.format(&#10;                    logbook.stream, halloffame))&#10;                for p in population:&#10;                    logging.debug(&quot;0, {}, {}, {}&quot;.format(gen, p, p.fitness))&#10;                for h in halloffame:&#10;                    logging.debug(&quot;-1, {}, {}, {}&quot;.format(gen, h, h.fitness))&#10;    &#10;    else:  # Run while population fitness variance is less than limit&#10;        counter = 0&#10;        gen = 1&#10;        &#10;        while counter &lt; termination_args:&#10;            logging.debug(&quot;Into while, counter = {}&quot;.format(counter))&#10;            &#10;            offspring = toolbox.select(population, len(population))&#10;            offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)&#10;            &#10;            for ind in offspring:&#10;                if str(ind) in visited_inds:&#10;                    ind.fitness.values = visited_inds[str(ind)]&#10;            &#10;            invalid_ind = [ind for ind in offspring &#10;                          if (not ind.fitness.valid) and (str(ind) not in visited_inds)]&#10;            fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)&#10;            &#10;            for ind, fit in zip(invalid_ind, fitnesses):&#10;                ind.fitness.values = fit&#10;                visited_inds[str(ind)] = fit&#10;            &#10;            if halloffame is not None:&#10;                halloffame.update(offspring)&#10;&#10;            population[:] = offspring&#10;&#10;            record = stats.compile(population) if stats else {}&#10;            logbook.record(gen=gen, nevals=len(invalid_ind), **record)&#10;            &#10;            cp = dict(population=population, generation=gen, halloffame=halloffame,&#10;                     logbook=logbook, rndstate=random.getstate())&#10;            &#10;            with open(checkpoint_file, &quot;wb&quot;) as cp_file:&#10;                pickle.dump(cp, cp_file)&#10;            &#10;            logging.info(&quot;Generation {} Stored at {}&quot;.format(&#10;                gen, time.strftime(&quot;%H:%M:%S&quot;, time.localtime())))&#10;            &#10;            if verbose:&#10;                printf(&quot;Logbookstream: {}\nhalloffame: {}\n&quot;.format(&#10;                    logbook.stream, halloffame))&#10;                for p in population:&#10;                    logging.debug(&quot;0, {}, {}, {}&quot;.format(gen, p, p.fitness))&#10;                for h in halloffame:&#10;                    logging.debug(&quot;-1, {}, {}, {}&quot;.format(gen, h, h.fitness))&#10;            &#10;            if term == &quot;fitmin&quot;:&#10;                if float(logbook.select(&quot;min&quot;)[-1]) &lt;= ngen:&#10;                    counter += 1&#10;                else:&#10;                    counter = 0&#10;            elif term == &quot;fitvar&quot;:&#10;                if math.pow(float(logbook.select(&quot;std&quot;)[-1]), 2) &lt;= ngen:&#10;                    counter += 1&#10;                else:&#10;                    counter = 0&#10;            elif term == &quot;fitavg&quot;:&#10;                if float(logbook.select(&quot;avg&quot;)[-1]) &lt;= ngen:&#10;                    counter += 1&#10;                else:&#10;                    counter = 0&#10;            else:&#10;                logging.info(&quot;Unknown GA configuration value: '{}'... Exiting&quot;.format(term))&#10;                counter = termination_args&#10;&#10;            logging.debug(&quot;Generation fitness variance = {}, counter is now: {}&quot;.format(&#10;                math.pow(float(logbook.select(&quot;std&quot;)[-1]), 2), counter))&#10;            gen += 1&#10;&#10;    logging.info(&quot;{}\n&quot;.format(logbook.stream))&#10;    return population, logbook&#10;&#10;&#10;def run():&#10;    &quot;&quot;&quot;Main GA execution function&quot;&quot;&quot;&#10;    global ga_config, ga_params, termination_crit, termination_args&#10;    global crossover_prob, mutation_prob, tournament_size, pop_num&#10;    global checkpoint_file, transformer&#10;    &#10;    seed = 1234567&#10;&#10;    distance_type_id = ga_config['distance_type']&#10;    logging.info(&quot;Crossover probability: {}, Mutation probability: {}, Tournament size: {}&quot;.format(&#10;        crossover_prob, mutation_prob, tournament_size))&#10;    logging.info(&quot;No. of population: {}, Random seed: {}, GA parameters: {}&quot;.format(&#10;        pop_num, seed, ga_params))&#10;    logging.info(&quot;Distance type - [{}]\t Termination criterion - [{}] - args [{}]\n&quot;.format(&#10;        distance_type_id, termination_crit, termination_args))&#10;    logging.info(&quot;Begin at: {}&quot;.format(time.strftime(&quot;%H:%M:%S&quot;, time.localtime())))&#10;    &#10;    random.seed(seed)&#10;    ga_parameters = ga_utils.create_parameters(ga_params)&#10;    transformer = Transformer(ga_parameters)&#10;&#10;    # DEAP class creators&#10;    creator.create(&quot;FitnessMin&quot;, base.Fitness, weights=(-1.0,))&#10;    creator.create(&quot;Individual&quot;, list, fitness=creator.FitnessMin)&#10;&#10;    # DEAP method definitions&#10;    toolbox = base.Toolbox()&#10;    toolbox.register(&quot;individual&quot;, tools.initIterate, creator.Individual,&#10;                     make_random_parameters)&#10;    toolbox.register(&quot;population&quot;, tools.initRepeat, list, toolbox.individual)&#10;    toolbox.register(&quot;evaluate&quot;, obj_func)&#10;    toolbox.register(&quot;mate&quot;, cxUniform, indpb=crossover_prob)&#10;    toolbox.register(&quot;mutate&quot;, custom_mutate, indpb=mutation_prob)&#10;    toolbox.register(&quot;select&quot;, tools.selTournament, tournsize=tournament_size)&#10;    toolbox.register(&quot;map&quot;, queue_map)&#10;&#10;    pop = toolbox.population(n=pop_num)&#10;&#10;    print(&quot;\n\n\n {} \n\n\n\n&quot;.format(pop))&#10;&#10;    hof = tools.HallOfFame(pop_num)&#10;    stats = tools.Statistics(lambda ind: ind.fitness.values)&#10;    stats.register(&quot;avg&quot;, np.mean)&#10;    stats.register(&quot;std&quot;, np.std)&#10;    stats.register(&quot;min&quot;, np.min)&#10;    stats.register(&quot;max&quot;, np.max)&#10;    stats.register(&quot;ts&quot;, timestamp)&#10;&#10;    start_time = time.time()&#10;    pop, log = eaSimpleExtended(pop, toolbox, cxpb=crossover_prob, mutpb=mutation_prob,&#10;                               term=termination_crit, ngen=num(termination_args),&#10;                               stats=stats, halloffame=hof, verbose=True, checkpoint=None)&#10;    end_time = time.time()&#10;&#10;    fitnesses = [str(p.fitness.values[0]) for p in pop]&#10;    logging.info(&quot;Logbook: \n{}&quot;.format(log))&#10;    logging.info(&quot;\n Hall of Fame: \n&quot;)&#10;    logging.info(&quot;End at: {}&quot;.format(time.strftime(&quot;%H:%M:%S&quot;, time.localtime())))&#10;    &#10;    for h in hof:&#10;        logging.debug(&quot;-1, {}, {}, {}&quot;.format(-1, h, h.fitness))&#10;    &#10;    producer = KafkaProducer(&#10;        bootstrap_servers=broker_ip,&#10;        value_serializer=lambda v: json.dumps(v).encode('utf-8')&#10;    )&#10;    &#10;    #for h in hof:&#10;    producer.send(output_topic, &quot;proc_ID: {}, barrier_heights: {}, fitness_score: {}&quot;.format(proc_id, hof[0], hof[0].fitness.values[0]))&#10;&#10;&#10;def rm_main(data):&#10;    &quot;&quot;&quot;Main entry point for the GA optimizer&quot;&quot;&quot;&#10;    global ga_config, ga_params, broker_ip, config_topic, input_topic, output_topic&#10;    global termination_crit, termination_args, crossover_prob, mutation_prob&#10;    global tournament_size, pop_num, checkpoint_file, consumer, proc_id&#10;&#10;    broker_ip = 'server.crexdata.eu:9192'&#10;    config_topic = 'ga_config'&#10;    input_topic = 'ga_in'&#10;    output_topic = 'ga_out'&#10;&#10;    print(&quot;Broker IP: {}, Config Topic: {}, Input Topic: {}, Output Topic: {}&quot;.format(&#10;        broker_ip, config_topic, input_topic, output_topic))&#10;&#10;    configurator = KafkaConsumer(&#10;        config_topic,&#10;        bootstrap_servers=broker_ip,&#10;        value_deserializer=lambda m: json.loads(m.decode('utf-8')),&#10;        auto_offset_reset='latest',&#10;        enable_auto_commit=True&#10;    )&#10;&#10;    consumer = KafkaConsumer(&#10;        input_topic,&#10;        bootstrap_servers=broker_ip,&#10;        value_deserializer=lambda m: json.loads(m.decode('utf-8')),&#10;        auto_offset_reset='latest',&#10;        enable_auto_commit=True&#10;    )&#10;&#10;    while True:&#10;        for message1 in configurator:&#10;            try:&#10;                print(&quot;Initiating simulations with configurations: {}&quot;.format(message1.value))&#10;                &#10;                # The config is inside 'config_topic' key and is a JSON string&#10;                config_string = message1.value['config_topic']&#10;                received = json.loads(config_string)  # Parse the JSON string&#10;                &#10;                proc_id = received['process_id']&#10;                ga_config = received['ga_config.json']&#10;                print(&quot;Loaded configuration: {}&quot;.format(ga_config))&#10;                &#10;                termination_crit = ga_config['termination_crit']&#10;                termination_args = ga_config['termination_args']&#10;                crossover_prob = float(ga_config['crossover_prob'])&#10;                mutation_prob = float(ga_config['mutation_prob'])&#10;                tournament_size = int(ga_config['tournament_size'])&#10;                pop_num = int(ga_config['pop_num'])&#10;                checkpoint_file = os.path.join(experiment_folder, &quot;ga_checkpoint.pkl&quot;)&#10;                &#10;                # The chromosome def is inside ga_config.json, not at root level&#10;                ga_params = ga_config['ga_chromosome_def']&#10;                print(&quot;Loaded chromosome definitions: {}&quot;.format(ga_params))&#10;                &#10;                run()&#10;                print(&quot;\nOptimization Completed!\n&quot;)&#10;                return pd.DataFrame({'values': [3, 5, 77, 8]})&#10;&#10;            except KeyError as e:&#10;                print(f&quot;KeyError: {e}&quot;)&#10;                print(&quot;Malformed message! Ignoring...&quot;)&#10;            except Exception as e:&#10;                print(f&quot;Unexpected error: {e}&quot;)&#10;"/>
        <parameter key="notebook_cell_tag_filter" value=""/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="use_macros" value="false"/>
      </operator>
      <connect from_op="Retrieve kafka-no-auth - 2" from_port="output" to_op="Read Kafka Topic" to_port="connection"/>
      <connect from_op="Read Kafka Topic" from_port="output data" to_op="Generate ga_config" to_port="input 1"/>
      <connect from_op="Generate ga_config" from_port="output 1" to_op="Execute Python (2)" to_port="input 1"/>
      <connect from_op="Execute Python (2)" from_port="output 1" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <description align="left" color="yellow" colored="false" height="344" resized="true" width="932" x="11" y="215">1- Read UPB-CREXDATA-Flooding-Optimizer-Input Kafka topic&lt;br&gt;&lt;br&gt;2- Use kafka-topic to generate ga_config&lt;br&gt;&lt;br&gt;3- Start Optimizer with ga_config as input&lt;br&gt;&lt;br&gt;4- Optimizer will send a POST request to: /api/v1/services/barrieroptimization/create-barrier-height-simulations (list of barrier heights from optimizer)&lt;br&gt;&lt;br&gt;5- and will wait for the response (list of max water levels).&lt;br&gt;Received 6 fitness scores from HTTP: [0.8656024574252954, 0.029544857248441247, 0.13295234382640586, 0.6123848865108802, 0.4776328201987664, 0.5863267397014968]&lt;br&gt;&lt;br&gt;6 - The it will resume its execution to produce hallOfFame:&lt;br&gt;Logbookstream: 9 6 0.335788 0.267542 0.0295449 0.865602 1761721752.3142247&lt;br&gt;halloffame: [[2.8131013156129843, 3.5155014702447414, 3.4057007548428357, 1.4929724262866089], [1.12, 2.3, 2.42, 1.57], [2.882774173704998, 3.3865225302358692, 3.1789664676218834, 1.4780865337792595], [3.073441419344871, 3.1408235578972246, 2.159814214349862, 1.53], [2.81, 3.417743816666012, 3.316427006419364, 1.4283308833069894], [2.916532732170558, 3.36, 3.388219431841713, 1.4780865337792595], [1.76, 2.04, 2.37, 2.99], [3.9270067806191022, 2.83, 3.3779003450375886, 2.6], [2.81, 3.36, 3.568592689365606, 1.4929724262866089], [2.81, 3.36, 3.48, 1.53]]&lt;br&gt;</description>
    </process>
  </operator>
</process>
