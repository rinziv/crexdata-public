<?xml version="1.0" encoding="UTF-8"?><process version="10.5.000">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="10.5.000" expanded="true" name="Process" origin="GENERATED_TUTORIAL">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="open_file" compatibility="10.5.000" expanded="true" height="68" name="QA LLM" width="90" x="983" y="187">
        <parameter key="resource_type" value="repository blob entry"/>
        <parameter key="repository_entry" value="../models/Llama-3.2-3B-Instruct-UD-Q5_K_XL.gguf"/>
      </operator>
      <operator activated="true" class="retrieve" compatibility="10.5.000" expanded="true" height="68" name="Retrieve server-crexdata-eu-no-auth" width="90" x="313" y="34">
        <parameter key="repository_entry" value="/Connections/server-crexdata-eu-no-auth"/>
      </operator>
      <operator activated="true" class="kafka_connector:read_kafka_topic" compatibility="0.3.002" expanded="true" height="82" name="Read Kafka Topic" width="90" x="447" y="34">
        <parameter key="kafka_topic" value="text_mining_test_topic_2"/>
        <parameter key="offset_strategy" value="earliest"/>
        <parameter key="retrieval_time_out" value="5"/>
        <parameter key="get_all" value="true"/>
        <parameter key="number_of_records" value="100"/>
        <parameter key="collection_strategy" value="duration"/>
        <parameter key="counter" value="100"/>
        <parameter key="time_out" value="120"/>
        <parameter key="polling_time_out" value="5"/>
        <parameter key="api_timeout" value="10"/>
      </operator>
      <operator activated="true" class="blending:select_attributes" compatibility="10.5.000" expanded="true" height="82" name="Select Attributes" width="90" x="581" y="34">
        <parameter key="type" value="include attributes"/>
        <parameter key="attribute_filter_type" value="one attribute"/>
        <parameter key="select_attribute" value="value"/>
        <parameter key="select_subset" value=""/>
        <parameter key="also_apply_to_special_attributes_(id,_label..)" value="false"/>
      </operator>
      <operator activated="true" class="text:data_to_documents" compatibility="10.1.000" expanded="true" height="68" name="Data to Documents" width="90" x="715" y="34">
        <parameter key="select_attributes_and_weights" value="false"/>
        <list key="specify_weights"/>
      </operator>
      <operator activated="true" class="text:json_to_data" compatibility="10.1.000" expanded="true" height="82" name="JSON To Data" width="90" x="849" y="34">
        <parameter key="ignore_arrays" value="false"/>
        <parameter key="limit_attributes" value="false"/>
        <parameter key="skip_invalid_documents" value="false"/>
        <parameter key="guess_data_types" value="true"/>
        <parameter key="keep_missing_attributes" value="false"/>
        <parameter key="missing_values_aliases" value=", null, NaN, missing"/>
        <parameter key="preserve_attribute_order" value="false"/>
      </operator>
      <operator activated="true" class="filter_examples" compatibility="10.5.000" expanded="true" height="103" name="Filter nones" width="90" x="983" y="34">
        <parameter key="parameter_string" value="event_class != none"/>
        <parameter key="parameter_expression" value=""/>
        <parameter key="condition_class" value="attribute_value_filter"/>
        <parameter key="invert_filter" value="false"/>
        <list key="filters_list"/>
        <parameter key="filters_logic_and" value="true"/>
        <parameter key="filters_check_metadata" value="true"/>
      </operator>
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="82" name="QA inference" width="90" x="1117" y="34">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;Custom Python Transformer&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;temperature&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Temperature parameter for gpt inference\. Value should be greater zero\. (float) default 0\.7&quot;,&#10;      &quot;value&quot;: 0\.7&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;top_k&quot;,&#10;      &quot;type&quot;: &quot;integer&quot;,&#10;      &quot;description&quot;: &quot;top k parameter for gpt inference\. Value should be greater zero [1-inf]\. (float) default 10&quot;,&#10;      &quot;value&quot;: 10&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;top_p&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;top p parameter for gpt inference\. (float) defaults 0\.95&quot;,&#10;      &quot;value&quot;: 0\.95&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;min_p&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;min p parameter for gpt inference\. (float) defaults 0\.05&quot;,&#10;      &quot;value&quot;: 0\.05&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;max_tokens&quot;,&#10;      &quot;type&quot;: &quot;integer&quot;,&#10;      &quot;description&quot;: &quot;Max generation tokens parameter for gpt inference\. Multiples of 2, larger value will allow QA to output more tokens\. (integer) defaults 4096&quot;,&#10;      &quot;value&quot;: 4096&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;repeat_penalty&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;reapetition penalty parameter for gpt inference\. (float) defaults 1\.1&quot;,&#10;      &quot;value&quot;: 1\.1&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;qa_response_language&quot;,&#10;      &quot;type&quot;: &quot;category&quot;,&#10;      &quot;description&quot;: &quot;Choose a language to receive response in\. English, Spanish, Catalan, German&quot;,&#10;      &quot;categories&quot;: [&#10;        &quot;English&quot;,&#10;        &quot;Spanish&quot;,&#10;        &quot;Catalan&quot;,&#10;        &quot;German&quot;&#10;      ],&#10;      &quot;value&quot;: &quot;English&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;summarise&quot;,&#10;      &quot;type&quot;: &quot;boolean&quot;,&#10;      &quot;description&quot;: &quot;Summarise the incident\. If True, query parameter not required\.&quot;,&#10;      &quot;value&quot;: true&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;query&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;An example of a categorical parameter\.&quot;,&#10;      &quot;value&quot;: &quot;Tell me about the incident in the messages\.&quot;,&#10;      &quot;optional&quot;: true&#10;    }&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;data&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;model&quot;,&#10;      &quot;type&quot;: &quot;file&quot;&#10;    }&#10;  ],&#10;  &quot;outputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;out&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;through&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ]&#10;}.from pandas import DataFrame&#10;from llama_cpp import Llama&#10;&#10;&#10;# Mandatory main function\. This example expects a single input followed by the&#10;# parameter dictionary\.&#10;def rm_main(data, mod, parameters):&#10;&#9;model = Llama(model_path=mod\.name,n_ctx=8192,use_mmap=False) &#10;&#10;&#9;template_texts = &quot;&quot;&#10;&#9;for i, text in enumerate(data['tweet_text']\.to_list()):&#10;&#9;&#9;template_texts += f'{i+1}\. {text} \\n'&#10;&#10;&#9;if parameters['summarise']:&#10;&#9;&#9;conversation = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &#10;&#9;                     &quot;The documents below describe a developing disaster event\. &quot;&#10;&#9;                     &quot;Based on these documents, write a brief summary in the form of a paragraph, &quot; &#10;&#9;                     &quot;highlighting the most crucial information\. &quot;&#10;&#9;                     f&quot;Reply ONLY in {parameters['qa_response_language']} regardless of the language in the documents\.\\n&quot;&#10;&#9;                     f&quot;Documents: {template_texts\.strip()}&quot;}]&#10;&#9;&#9;query = &quot;none&quot;&#10;&#9;else:&#10;&#9;&#9;conversation = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &#10;&#9;                     &quot;For the following query and documents, &quot;&#10;&#9;                     &quot;try to answer the given query using only the documents\. &quot;&#10;&#9;                     &quot;If the answer is not in the documents, state that\. &quot;&#10;&#9;                     &quot;Refer to documents in your response using their numbers\. &quot;&#10;&#9;                     f&quot;Reply ONLY in {parameters['qa_response_language']} regardless of the language in the documents\. &quot;&#10;&#9;                     f&quot;\\nQuery:\\n{parameters['query']}\\nDocuments:\\n{template_texts\.strip()}&quot;}]&#10;&#9;&#9;query = parameters['query']&#10;&#9;&#10;&#9;model\.reset()&#10;&#9;res = model\.create_chat_completion(conversation, top_k=int(parameters['top_k']), top_p=float(parameters['top_p']), min_p=float(parameters['min_p']), temperature=float(parameters['temperature']), max_tokens=int(parameters['max_tokens']), repeat_penalty=float(parameters['repeat_penalty']))&#10;&#9;qa_response = res[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]&#10;&#9;&#10;&#9;response = DataFrame({&#10;&#9;&#9;'Query': [query],&#10;&#9;&#9;'Response': [qa_response]&#10;&#9;})&#10;&#9;&#10;&#9;return response, data&#10;&#10;&#10;"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="temperature" value="0.7"/>
        <parameter key="top_k" value="10"/>
        <parameter key="top_p" value="0.95"/>
        <parameter key="min_p" value="0.05"/>
        <parameter key="max_tokens" value="4096"/>
        <parameter key="repeat_penalty" value="1.1"/>
        <parameter key="qa_response_language" value="English"/>
        <parameter key="summarise" value="false"/>
        <parameter key="query" value="Tell me about the incident in the messages."/>
        <description align="center" color="transparent" colored="false" width="126">Custom Operator</description>
      </operator>
      <connect from_op="QA LLM" from_port="file" to_op="QA inference" to_port="model"/>
      <connect from_op="Retrieve server-crexdata-eu-no-auth" from_port="output" to_op="Read Kafka Topic" to_port="connection"/>
      <connect from_op="Read Kafka Topic" from_port="output data" to_op="Select Attributes" to_port="example set input"/>
      <connect from_op="Select Attributes" from_port="example set output" to_op="Data to Documents" to_port="example set"/>
      <connect from_op="Data to Documents" from_port="documents" to_op="JSON To Data" to_port="documents 1"/>
      <connect from_op="JSON To Data" from_port="example set" to_op="Filter nones" to_port="example set input"/>
      <connect from_op="Filter nones" from_port="example set output" to_op="QA inference" to_port="data"/>
      <connect from_op="QA inference" from_port="out" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <description align="center" color="yellow" colored="false" height="96" resized="true" width="507" x="451" y="155">Ensure Kafka Topic has tweet messages in json format with field name &amp;quot;tweet_text&amp;quot;, and event prediction field: &amp;quot;event_class&amp;quot; - fire, flood, none</description>
    </process>
  </operator>
</process>
